const { pipeline } = require('@xenova/transformers');

class HuggingFaceLLMService {
  constructor() {
    this.llmModel = process.env.HF_LLM_MODEL || 'microsoft/DialoGPT-medium';
    this.embeddingModel = process.env.HF_EMBEDDING_MODEL || 'sentence-transformers/all-MiniLM-L6-v2';
    this.generator = null;
    this.embedder = null;
    this.initialized = false;
  }

  /**
   * Initialize the models
   */
  async initialize() {
    if (this.initialized) return;

    try {
      console.log('ðŸ”„ Initializing Hugging Face models...');
      
      // Initialize text generation pipeline
      this.generator = await pipeline('text-generation', this.llmModel, {
        quantized: true, // Use quantized models for better performance
        device: 'cpu' // Use CPU for compatibility
      });

      // Initialize feature extraction pipeline for embeddings
      this.embedder = await pipeline('feature-extraction', this.embeddingModel, {
        quantized: true,
        device: 'cpu'
      });
      
      this.initialized = true;
      console.log('âœ… Hugging Face models initialized successfully');
    } catch (error) {
      console.error('âŒ Failed to initialize Hugging Face models:', error);
      throw error;
    }
  }

  /**
   * Generate embeddings using feature extraction
   */
  async generateEmbedding(text) {
    try {
      if (!this.initialized) await this.initialize();
      
      const result = await this.embedder(text, { pooling: 'mean', normalize: true });
      return Array.from(result.data);
    } catch (error) {
      console.error('âŒ Hugging Face embedding generation failed:', error);
      return this.generateSimpleEmbedding(text);
    }
  }

  /**
   * Generate response using Hugging Face model
   */
  async generateResponse(query, context, language = 'en') {
    try {
      if (!this.initialized) await this.initialize();

      const systemPrompt = this.createSystemPrompt(language);
      const fullPrompt = `${systemPrompt}\n\nContext:\n${context}\n\nUser Query: ${query}\n\nResponse:`;

      const result = await this.generator(fullPrompt, {
        max_length: 500,
        temperature: 0.7,
        do_sample: true,
        pad_token_id: this.generator.tokenizer.eos_token_id
      });

      // Extract the generated response
      let response = result[0].generated_text;
      
      // Clean up the response
      response = response.replace(fullPrompt, '').trim();
      
      // Remove any remaining context or user query
      response = response.split('User Query:')[0].split('Context:')[0].trim();
      
      return response || this.generateFallbackResponse(context, language);
    } catch (error) {
      console.error('âŒ Hugging Face LLM generation failed:', error);
      return this.generateFallbackResponse(context, language);
    }
  }

  /**
   * Create system prompt for the LLM
   */
  createSystemPrompt(language) {
    const prompts = {
      en: `You are a helpful assistant for RuralConnect, a government scheme information chatbot. 
You help citizens understand government welfare schemes in a clear, friendly, and accurate manner.

Guidelines:
- Provide accurate information based only on the context provided
- Be concise but comprehensive
- Use simple, easy-to-understand language
- If the query is not related to government schemes, politely redirect
- Always mention relevant contact information and websites
- If you don't have enough information, say so clearly

Respond in a helpful, informative manner.`,

      hi: `à¤†à¤ª RuralConnect à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤¸à¤¹à¤¾à¤¯à¤• à¤¹à¥ˆà¤‚, à¤œà¥‹ à¤¸à¤°à¤•à¤¾à¤°à¥€ à¤¯à¥‹à¤œà¤¨à¤¾ à¤¸à¥‚à¤šà¤¨à¤¾ à¤šà¥ˆà¤Ÿà¤¬à¥‰à¤Ÿ à¤¹à¥ˆà¥¤
à¤†à¤ª à¤¨à¤¾à¤—à¤°à¤¿à¤•à¥‹à¤‚ à¤•à¥‹ à¤¸à¤°à¤•à¤¾à¤°à¥€ à¤•à¤²à¥à¤¯à¤¾à¤£ à¤¯à¥‹à¤œà¤¨à¤¾à¤“à¤‚ à¤•à¥‹ à¤¸à¤®à¤à¤¨à¥‡ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

à¤¦à¤¿à¤¶à¤¾à¤¨à¤¿à¤°à¥à¤¦à¥‡à¤¶:
- à¤•à¥‡à¤µà¤² à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤¿à¤ à¤—à¤ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤•à¥‡ à¤†à¤§à¤¾à¤° à¤ªà¤° à¤¸à¤Ÿà¥€à¤• à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¦à¥‡à¤‚
- à¤¸à¤‚à¤•à¥à¤·à¤¿à¤ªà¥à¤¤ à¤²à¥‡à¤•à¤¿à¤¨ à¤µà¥à¤¯à¤¾à¤ªà¤• à¤°à¤¹à¥‡à¤‚
- à¤¸à¤°à¤², à¤¸à¤®à¤à¤¨à¥‡ à¤®à¥‡à¤‚ à¤†à¤¸à¤¾à¤¨ à¤­à¤¾à¤·à¤¾ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚
- à¤¯à¤¦à¤¿ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤¸à¤°à¤•à¤¾à¤°à¥€ à¤¯à¥‹à¤œà¤¨à¤¾à¤“à¤‚ à¤¸à¥‡ à¤¸à¤‚à¤¬à¤‚à¤§à¤¿à¤¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ, à¤¤à¥‹ à¤µà¤¿à¤¨à¤®à¥à¤°à¤¤à¤¾ à¤¸à¥‡ à¤ªà¥à¤¨à¤°à¥à¤¨à¤¿à¤°à¥à¤¦à¥‡à¤¶à¤¿à¤¤ à¤•à¤°à¥‡à¤‚
- à¤¹à¤®à¥‡à¤¶à¤¾ à¤ªà¥à¤°à¤¾à¤¸à¤‚à¤—à¤¿à¤• à¤¸à¤‚à¤ªà¤°à¥à¤• à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤”à¤° à¤µà¥‡à¤¬à¤¸à¤¾à¤‡à¤Ÿà¥‹à¤‚ à¤•à¤¾ à¤‰à¤²à¥à¤²à¥‡à¤– à¤•à¤°à¥‡à¤‚

à¤¸à¤¹à¤¾à¤¯à¤•, à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€à¤ªà¥‚à¤°à¥à¤£ à¤¤à¤°à¥€à¤•à¥‡ à¤¸à¥‡ à¤œà¤µà¤¾à¤¬ à¤¦à¥‡à¤‚à¥¤`,

      ta: `à®¨à¯€à®™à¯à®•à®³à¯ RuralConnect à®•à¯à®•à®¾à®© à®‰à®¤à®µà®¿à®¯à®¾à®³à®°à¯, à®…à®°à®šà¯ à®¤à®¿à®Ÿà¯à®Ÿ à®¤à®•à®µà®²à¯ à®…à®°à®Ÿà¯à®Ÿà¯ˆ à®ªà¯‹à®Ÿà¯.
à®¨à¯€à®™à¯à®•à®³à¯ à®•à¯à®Ÿà®¿à®®à®•à¯à®•à®³à¯à®•à¯à®•à¯ à®…à®°à®šà¯ à®¨à®²à®¤à¯à®¤à®¿à®Ÿà¯à®Ÿà®™à¯à®•à®³à¯ˆà®ªà¯ à®ªà¯à®°à®¿à®¨à¯à®¤à¯à®•à¯Šà®³à¯à®³ à®‰à®¤à®µà¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯.

à®µà®´à®¿à®•à®¾à®Ÿà¯à®Ÿà¯à®¤à®²à¯à®•à®³à¯:
- à®µà®´à®™à¯à®•à®ªà¯à®ªà®Ÿà¯à®Ÿ à®šà¯‚à®´à®²à®¿à®©à¯ à®…à®Ÿà®¿à®ªà¯à®ªà®Ÿà¯ˆà®¯à®¿à®²à¯ à®®à®Ÿà¯à®Ÿà¯à®®à¯‡ à®¤à¯à®²à¯à®²à®¿à®¯à®®à®¾à®© à®¤à®•à®µà®²à¯ˆ à®µà®´à®™à¯à®•à¯à®™à¯à®•à®³à¯
- à®šà¯à®°à¯à®•à¯à®•à®®à®¾à®• à®†à®©à®¾à®²à¯ à®µà®¿à®°à®¿à®µà®¾à®• à®‡à®°à¯à®™à¯à®•à®³à¯
- à®Žà®³à®¿à®¯, à®ªà¯à®°à®¿à®¨à¯à®¤à¯à®•à¯Šà®³à¯à®³ à®Žà®³à®¿à®¤à®¾à®© à®®à¯Šà®´à®¿à®¯à¯ˆà®ªà¯ à®ªà®¯à®©à¯à®ªà®Ÿà¯à®¤à¯à®¤à¯à®™à¯à®•à®³à¯
- à®•à¯‡à®³à¯à®µà®¿ à®…à®°à®šà¯ à®¤à®¿à®Ÿà¯à®Ÿà®™à¯à®•à®³à¯à®Ÿà®©à¯ à®¤à¯Šà®Ÿà®°à¯à®ªà¯à®Ÿà¯ˆà®¯à®¤à®¾à®• à®‡à®²à¯à®²à®¾à®µà®¿à®Ÿà¯à®Ÿà®¾à®²à¯, à®®à®°à®¿à®¯à®¾à®¤à¯ˆà®¯à¯à®Ÿà®©à¯ à®®à®±à¯à®µà®´à®¿à®•à®¾à®Ÿà¯à®Ÿà¯à®™à¯à®•à®³à¯
- à®Žà®ªà¯à®ªà¯‹à®¤à¯à®®à¯ à®¤à¯Šà®Ÿà®°à¯à®ªà¯à®Ÿà¯ˆà®¯ à®¤à¯Šà®Ÿà®°à¯à®ªà¯ à®¤à®•à®µà®²à¯ à®®à®±à¯à®±à¯à®®à¯ à®µà®²à¯ˆà®¤à¯à®¤à®³à®™à¯à®•à®³à¯ˆà®•à¯ à®•à¯à®±à®¿à®ªà¯à®ªà®¿à®Ÿà¯à®™à¯à®•à®³à¯

à®‰à®¤à®µà®¿à®¯à®¾à®©, à®¤à®•à®µà®²à®±à®¿à®¨à¯à®¤ à®®à¯à®±à¯ˆà®¯à®¿à®²à¯ à®ªà®¤à®¿à®²à®³à®¿à®•à¯à®•à®µà¯à®®à¯à¥¤`
    };

    return prompts[language] || prompts.en;
  }

  /**
   * Generate fallback response when LLM fails
   */
  generateFallbackResponse(context, language) {
    const fallbackMessages = {
      en: "I apologize, but I'm having trouble processing your request right now. Please try again later or contact our support team.",
      hi: "à¤®à¥ˆà¤‚ à¤•à¥à¤·à¤®à¤¾ à¤šà¤¾à¤¹à¤¤à¤¾ à¤¹à¥‚à¤‚, à¤²à¥‡à¤•à¤¿à¤¨ à¤®à¥à¤à¥‡ à¤†à¤ªà¤•à¥‡ à¤…à¤¨à¥à¤°à¥‹à¤§ à¤•à¥‹ à¤¸à¤‚à¤¸à¤¾à¤§à¤¿à¤¤ à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚ à¤•à¤ à¤¿à¤¨à¤¾à¤ˆ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤ à¤•à¥ƒà¤ªà¤¯à¤¾ à¤¬à¤¾à¤¦ à¤®à¥‡à¤‚ à¤ªà¥à¤¨à¤ƒ à¤ªà¥à¤°à¤¯à¤¾à¤¸ à¤•à¤°à¥‡à¤‚ à¤¯à¤¾ à¤¹à¤®à¤¾à¤°à¥€ à¤¸à¤¹à¤¾à¤¯à¤¤à¤¾ à¤Ÿà¥€à¤® à¤¸à¥‡ à¤¸à¤‚à¤ªà¤°à¥à¤• à¤•à¤°à¥‡à¤‚à¥¤",
      ta: "à®®à®©à¯à®©à®¿à®•à¯à®•à®µà¯à®®à¯, à®†à®©à®¾à®²à¯ à®‰à®™à¯à®•à®³à¯ à®•à¯‹à®°à®¿à®•à¯à®•à¯ˆà®¯à¯ˆ à®‡à®ªà¯à®ªà¯‹à®¤à¯ à®šà¯†à®¯à®²à¯à®ªà®Ÿà¯à®¤à¯à®¤à¯à®µà®¤à®¿à®²à¯ à®Žà®©à®•à¯à®•à¯ à®šà®¿à®•à¯à®•à®²à¯ à®‰à®³à¯à®³à®¤à¯. à®¤à®¯à®µà¯à®šà¯†à®¯à¯à®¤à¯ à®ªà®¿à®±à®•à¯ à®®à¯€à®£à¯à®Ÿà¯à®®à¯ à®®à¯à®¯à®±à¯à®šà®¿à®•à¯à®•à®µà¯à®®à¯ à®…à®²à¯à®²à®¤à¯ à®Žà®™à¯à®•à®³à¯ à®†à®¤à®°à®µà¯ à®•à¯à®´à¯à®µà¯ˆà®¤à¯ à®¤à¯Šà®Ÿà®°à¯à®ªà¯ à®•à¯Šà®³à¯à®³à®µà¯à®®à¯à¥¤"
    };

    return fallbackMessages[language] || fallbackMessages.en;
  }

  /**
   * Simple embedding fallback for development
   */
  generateSimpleEmbedding(text) {
    const words = text.toLowerCase().split(/\s+/);
    const embedding = new Array(384).fill(0);
    
    words.forEach(word => {
      const hash = word.split('').reduce((a, b) => {
        a = ((a << 5) - a) + b.charCodeAt(0);
        return a & a;
      }, 0);
      const index = Math.abs(hash) % 384;
      embedding[index] += 1;
    });
    
    return embedding;
  }

  /**
   * Get model info
   */
  getModelInfo() {
    return {
      llmModel: this.llmModel,
      embeddingModel: this.embeddingModel,
      initialized: this.initialized
    };
  }
}

module.exports = HuggingFaceLLMService;
