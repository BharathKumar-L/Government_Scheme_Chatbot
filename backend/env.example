# Local LLM Configuration
# Choose one of the following options:

# Option 1: Ollama (requires Ollama installation)
USE_OLLAMA=false
OLLAMA_URL=http://localhost:11434
LOCAL_MODEL=llama2:7b
EMBEDDING_MODEL=nomic-embed-text

# Option 2: Hugging Face Transformers (requires @xenova/transformers)
USE_HUGGINGFACE=false
HF_LLM_MODEL=microsoft/DialoGPT-medium
HF_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Option 3: Simple Local LLM (default - no external dependencies)
# This is the default option that works out of the box

# Google Translate API (optional)
GOOGLE_TRANSLATE_API_KEY=your_google_translate_api_key_here

# Redis Configuration (optional for caching)
REDIS_URL=redis://localhost:6379

# Server Configuration
PORT=3001
NODE_ENV=development

# ChromaDB Configuration
# You can specify either full URL in CHROMA_HOST or host + CHROMA_PORT
CHROMA_HOST=http://localhost
CHROMA_PORT=8000

# Data ingestion
# If true, training will read from LOCAL_DATA_PATH (fallback: backend/data/scraped_schemes.json)
USE_LOCAL_DATA=true
LOCAL_DATA_PATH="D:\FINAL YEAR PROJECT\Datasets\updated_data.csv"

# Ingestion batch size for vector DB adds
BATCH_SIZE=1000

# Rate Limiting
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100